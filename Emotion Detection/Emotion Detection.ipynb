{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of instances:  35888\n",
      "instance length:  2304\n",
      "28709 train samples\n",
      "3589 test samples\n",
      "Epoch 1/5\n",
      "256/256 [==============================] - 161s 627ms/step - loss: 1.7836 - acc: 0.2609\n",
      "Epoch 2/5\n",
      "256/256 [==============================] - 159s 622ms/step - loss: 1.5465 - acc: 0.3918\n",
      "Epoch 3/5\n",
      "256/256 [==============================] - 160s 623ms/step - loss: 1.3634 - acc: 0.4766\n",
      "Epoch 4/5\n",
      "256/256 [==============================] - 159s 622ms/step - loss: 1.2543 - acc: 0.5201\n",
      "Epoch 5/5\n",
      "256/256 [==============================] - 160s 624ms/step - loss: 1.1683 - acc: 0.5567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#------------------------------\n",
    "#cpu - gpu configuration\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': 56} ) #max: 1 gpu, 56 cpu\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "class emotionDetection:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "        \n",
    "        #construct CNN structure\n",
    "        self.model = Sequential()\n",
    "\n",
    "        #1st convolution layer\n",
    "        self.model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "\n",
    "        #2nd convolution layer\n",
    "        self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        self.model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "        #3rd convolution layer\n",
    "        self.model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        self.model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        self.model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "\n",
    "        #fully connected neural networks\n",
    "        self.model.add(Dense(1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.2))\n",
    "\n",
    "        self.model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "        \n",
    "    def analysis(self, emotions):\n",
    "        objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        y_pos = np.arange(len(objects))\n",
    "\n",
    "        plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "        plt.xticks(y_pos, objects)\n",
    "        plt.ylabel('percentage')\n",
    "        plt.title('emotion')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def load_weights(self, weights_dir):\n",
    "        self.model.load_weights(weights_dir)\n",
    "\n",
    "    def train(self, x, y, batch_size, epochs):\n",
    "        gen = ImageDataGenerator()\n",
    "        train_generator = gen.flow(x, y, batch_size=batch_size)\n",
    "        self.model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs)\n",
    "\n",
    "    def predict(self, x):        \n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        score = self.model.evaluate(x, y)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', 100*score[1])\n",
    "        \n",
    "    def see_test_result(self, x):\n",
    "        predictions = self.model.predict(x)\n",
    "\n",
    "        index = 0\n",
    "        for i in predictions:\n",
    "            if index < 30 and index >= 20:\n",
    "                #print(i) #predicted scores\n",
    "                #print(y_test[index]) #actual scores\n",
    "\n",
    "                testing_img = np.array(x_test[index], 'float32')\n",
    "                testing_img = testing_img.reshape([48, 48]);\n",
    "\n",
    "                plt.gray()\n",
    "                plt.imshow(testing_img)\n",
    "                plt.show()\n",
    "\n",
    "                print(i)\n",
    "\n",
    "                emotion_analysis(i)\n",
    "                print(\"----------------------------------------------\")\n",
    "            index = index + 1\n",
    "        \n",
    "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "#read kaggle facial expression recognition challenge dataset (fer2013.csv)\n",
    "#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "\n",
    "with open(\"data/fer2013/fer2013.csv\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "lines = np.array(content)\n",
    "\n",
    "num_of_instances = lines.size\n",
    "print(\"number of instances: \",num_of_instances)\n",
    "print(\"instance length: \",len(lines[1].split(\",\")[1].split(\" \")))\n",
    "\n",
    "#------------------------------\n",
    "#initialize trainset and test set\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "#------------------------------\n",
    "#transfer train and test set data\n",
    "for i in range(1,num_of_instances):\n",
    "    try:\n",
    "        emotion, img, usage = lines[i].split(\",\")\n",
    "\n",
    "        val = img.split(\" \")\n",
    "\n",
    "        pixels = np.array(val, 'float32')\n",
    "\n",
    "        emotion = keras.utils.to_categorical(emotion, num_classes)\n",
    "\n",
    "        if 'Training' in usage:\n",
    "            y_train.append(emotion)\n",
    "            x_train.append(pixels)\n",
    "        elif 'PublicTest' in usage:\n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels)\n",
    "    except:\n",
    "        print(\"\",end=\"\")\n",
    "\n",
    "#------------------------------\n",
    "#data transformation for train and test sets\n",
    "x_train = np.array(x_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "x_test = np.array(x_test, 'float32')\n",
    "y_test = np.array(y_test, 'float32')\n",
    "\n",
    "x_train /= 255 #normalize inputs between [0, 1]\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "img_path = \"/home/roboi9/workspace/Keras/Emotion Detection/data/test/ammar.jpg\"\n",
    "weights_dir = 'data/facial_expression_model_weights.h5'\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "emo = emotionDetection()\n",
    "emo.train(x_train, y_train,batch_size, epochs)\n",
    "# emo.load_weights(weights_dir)\n",
    "# emo.evaluate(x_test, y_test)\n",
    "# emo.see_test_result(x_test)\n",
    "\n",
    "img = image.load_img(img_path, grayscale=True, target_size=(48, 48))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "x /= 255\n",
    "\n",
    "custom = emo.predict(x)\n",
    "emo.analysis(custom[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:82: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    }
   ],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "args = dict()\n",
    "args['detector'] = 'face_detection_model'\n",
    "args['embedding_model'] = 'openface_nn4.small2.v1.t7'\n",
    "args['confidence'] = 0.5\n",
    "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
    "modelPath = os.path.sep.join([args[\"detector\"],\n",
    "                              \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "\n",
    "def setTextBg(text, font, font_scale, thickness):\n",
    "    size = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "    text_width = size[0][0]\n",
    "    text_height = size[0][1]\n",
    "    \n",
    "    return text_width,text_height\n",
    "\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read() \n",
    "\n",
    "    # resize the frame to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if confidence > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "                \n",
    "           \n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "#             faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "#                 (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "#             embedder.setInput(faceBlob)\n",
    "#             vec = embedder.forward()\n",
    "\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            x = face\n",
    "            x = cv2.resize(x, dsize=(48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "            x = np.expand_dims(x, axis = 0)\n",
    "            x = np.expand_dims(x, axis = 4)\n",
    "            custom = emo.predict(x)\n",
    "            \n",
    "#             text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            if (startY - 10) > 10:\n",
    "                y = startY - 10\n",
    "                y1 = startY - 20\n",
    "                y2 = startY - 30\n",
    "                y3 = startY - 40\n",
    "            else:\n",
    "                y = endY + 10\n",
    "                y1 = endY + 20\n",
    "                y2 = endY + 30\n",
    "                y3 = endY + 40\n",
    "            \n",
    "#             y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "#             y1 = startY - 10 if startY - 10 > 10 else startY + 20\n",
    "#             y2 = startY - 10 if startY - 10 > 10 else startY + 30\n",
    "#             y3 = startY - 10 if startY - 10 > 10 else startY + 40\n",
    "#             y4 = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "#             y5 = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            \n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), (230, 0, 230), 2)\n",
    "            #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "            width, height = setTextBg(\"Angry : {:.2f} Disgust : {:.2f}\".format(custom[0][0],custom[0][1]), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "            cv2.rectangle(frame, (startX, y), (startX + width, y-height), (230, 0, 230), cv2.FILLED) #BASELINE\n",
    "            width, height = setTextBg(\"Fear : {:.2f} Happy : {:.2f}\".format(custom[0][2],custom[0][3]), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "            cv2.rectangle(frame, (startX, y1), (startX + width, y1-height), (230, 0, 230), cv2.FILLED) #BASELINE\n",
    "            width, height = setTextBg(\"Sad : {:.2f} Surprise : {:.2f}\".format(custom[0][4],custom[0][5]), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "            cv2.rectangle(frame, (startX, y2), (startX + width, y2-height), (230, 0, 230), cv2.FILLED) #BASELINE\n",
    "            width, height = setTextBg(\"Neutral : {:.2f}\".format(custom[0][6]), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "            cv2.rectangle(frame, (startX, y3), (startX + width, y3-height), (230, 0, 230), cv2.FILLED) #BASELINE\n",
    "            \n",
    "            cv2.putText(frame,\n",
    "                        \"Angry : {:.2f} Disgust : {:.2f}\".format(custom[0][0],custom[0][1]),                        \n",
    "                        (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)\n",
    "            cv2.putText(frame,\n",
    "                        \"Fear : {:.2f} Happy : {:.2f}\".format(custom[0][2],custom[0][3]),                        \n",
    "                        (startX, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)\n",
    "            cv2.putText(frame,\n",
    "                        \"Sad : {:.2f} Surprise : {:.2f}\".format(custom[0][4],custom[0][5]),                        \n",
    "                        (startX, y2), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)\n",
    "            cv2.putText(frame,\n",
    "                        \"Neutral : {:.2f}\".format(custom[0][6]),                        \n",
    "                        (startX, y3), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)\n",
    "    \n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
