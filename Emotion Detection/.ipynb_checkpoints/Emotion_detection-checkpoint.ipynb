{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of instances:  35888\n",
      "instance length:  2304\n",
      "28709 train samples\n",
      "3589 test samples\n",
      "x size :  (1, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#------------------------------\n",
    "#cpu - gpu configuration\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': 56} ) #max: 1 gpu, 56 cpu\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "#------------------------------\n",
    "#variables\n",
    "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "#------------------------------\n",
    "#read kaggle facial expression recognition challenge dataset (fer2013.csv)\n",
    "#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "\n",
    "with open(\"data/fer2013/fer2013.csv\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "lines = np.array(content)\n",
    "\n",
    "num_of_instances = lines.size\n",
    "print(\"number of instances: \",num_of_instances)\n",
    "print(\"instance length: \",len(lines[1].split(\",\")[1].split(\" \")))\n",
    "\n",
    "#------------------------------\n",
    "#initialize trainset and test set\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "#------------------------------\n",
    "#transfer train and test set data\n",
    "for i in range(1,num_of_instances):\n",
    "    try:\n",
    "        emotion, img, usage = lines[i].split(\",\")\n",
    "\n",
    "        val = img.split(\" \")\n",
    "\n",
    "        pixels = np.array(val, 'float32')\n",
    "\n",
    "        emotion = keras.utils.to_categorical(emotion, num_classes)\n",
    "\n",
    "        if 'Training' in usage:\n",
    "            y_train.append(emotion)\n",
    "            x_train.append(pixels)\n",
    "        elif 'PublicTest' in usage:\n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels)\n",
    "    except:\n",
    "        print(\"\",end=\"\")\n",
    "\n",
    "#------------------------------\n",
    "#data transformation for train and test sets\n",
    "x_train = np.array(x_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "x_test = np.array(x_test, 'float32')\n",
    "y_test = np.array(y_test, 'float32')\n",
    "\n",
    "x_train /= 255 #normalize inputs between [0, 1]\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "#------------------------------\n",
    "#construct CNN structure\n",
    "model = Sequential()\n",
    "\n",
    "#1st convolution layer\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#------------------------------\n",
    "#batch process\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "model.compile(loss='categorical_crossentropy'\n",
    "    , optimizer=keras.optimizers.Adam()\n",
    "    , metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "fit = False\n",
    "\n",
    "if fit == True:\n",
    "    #model.fit_generator(x_train, y_train, epochs=epochs) #train for all trainset\n",
    "    model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) #train for randomly selected one\n",
    "else:\n",
    "    model.load_weights('data/facial_expression_model_weights.h5') #load weights\n",
    "\n",
    "#------------------------------\n",
    "\"\"\"\n",
    "#overall evaluation\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])\n",
    "\"\"\"\n",
    "#------------------------------\n",
    "#function for drawing bar chart for emotion preditions\n",
    "def emotion_analysis(emotions):\n",
    "    plt.clf()\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "\n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def emotion_analysis_realtime(emotions):     \n",
    "    print(emotions)\n",
    "#     line1, = ax.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "#     plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    line1.set_ydata(emotion)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "#------------------------------\n",
    "\n",
    "monitor_testset_results = False\n",
    "\n",
    "if monitor_testset_results == True:\n",
    "    #make predictions for test set\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    index = 0\n",
    "    for i in predictions:\n",
    "        if index < 30 and index >= 20:\n",
    "            #print(i) #predicted scores\n",
    "            #print(y_test[index]) #actual scores\n",
    "\n",
    "            testing_img = np.array(x_test[index], 'float32')\n",
    "            testing_img = testing_img.reshape([48, 48]);\n",
    "\n",
    "            plt.gray()\n",
    "            plt.imshow(testing_img)\n",
    "            plt.show()\n",
    "\n",
    "            print(i)\n",
    "\n",
    "            emotion_analysis(i)\n",
    "            print(\"----------------------------------------------\")\n",
    "        index = index + 1\n",
    "\n",
    "#------------------------------\n",
    "#make prediction for custom image out of test set\n",
    "img = image.load_img(\"/home/roboi9/workspace/Keras/Emotion Detection/data/test/ammar.jpg\",\n",
    "                     grayscale=True, target_size=(48, 48))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "\n",
    "x /= 255\n",
    "print(\"x size : \",x.shape)\n",
    "# custom = model.predict(x)\n",
    "# emotion_analysis(custom[0])\n",
    "\n",
    "# x = np.array(x, 'float32')\n",
    "# x = x.reshape([48, 48]);\n",
    "\n",
    "# plt.gray()\n",
    "# plt.imshow(x)\n",
    "# plt.show()\n",
    "# #------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:85: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BarContainer' object has no attribute 'set_ydata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-23a08b414e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mcustom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0memotion_analysis_realtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#             x = np.array(x, 'float32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-300fbfa95ad6>\u001b[0m in \u001b[0;36memotion_analysis_realtime\u001b[0;34m(emotions)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m#     line1, = ax.bar(y_pos, emotions, align='center', alpha=0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m#     plt.bar(y_pos, emotions, align='center', alpha=0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mline1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ydata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BarContainer' object has no attribute 'set_ydata'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "args = dict()\n",
    "args['detector'] = 'face_detection_model'\n",
    "args['embedding_model'] = 'openface_nn4.small2.v1.t7'\n",
    "args['confidence'] = 0.5\n",
    "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
    "modelPath = os.path.sep.join([args[\"detector\"],\n",
    "                              \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read() \n",
    "\n",
    "    # resize the frame to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if confidence > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "                \n",
    "           \n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "#             faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "#                 (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "#             embedder.setInput(faceBlob)\n",
    "#             vec = embedder.forward()\n",
    "\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            x = face\n",
    "            x = cv2.resize(x, dsize=(48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "            x = np.expand_dims(x, axis = 0)\n",
    "            x = np.expand_dims(x, axis = 4)\n",
    "            custom = model.predict(x)\n",
    "            emotion_analysis(custom[0])\n",
    "\n",
    "#             x = np.array(x, 'float32')\n",
    "#             x = x.reshape([48, 48]);\n",
    "\n",
    "#             plt.gray()\n",
    "#             plt.imshow(x)\n",
    "#             plt.show()\n",
    "\n",
    "#             # perform classification to recognize the face\n",
    "#             preds = recognizer.predict_proba(vec)[0]\n",
    "#             j = np.argmax(preds)\n",
    "#             proba = preds[j]\n",
    "#             name = le.classes_[j]\n",
    "\n",
    "            # draw the bounding box of the face along with the\n",
    "            # associated probability\n",
    "#             text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "#             cv2.putText(frame, text, (startX, y),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
